# -*- coding: utf-8 -*-
"""Kurniati Laeli Munifah_Proyek 2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1CbWglb4cyqfoii-FJEBK9m2E6YPnBNI5

# Proyek Kedua : Membuat Model Machine Learning dengan Data Time Series
- Nama: Kurniati Laeli Munifah
- Email: k.laelimunifah@gmail.com
- Id Dicoding: klmunifah

## Impor dan Pemrosesan Awal Dataset
Dataset yang digunakan adalah dataset LSTM-Multivariate_pollution.csv (https://www.kaggle.com/datasets/rupakroy/lstm-datasets-multivariate-univariate) . File berformat csv yang kemudian dibaca dengan menggunakan library pandas. Secara keseluruhan terdapat 43800 data pada masing-masing dari 9 kolom. Kolom yang digunakan adalah kolom date dan press yang kemudian disimpan dalam tabel df. Setelah ditemukan tidak ada data yang kosong pada masing-masing kolom, data dapat melalui proses selanjutnya
"""

# Import Library
import numpy as np
import pandas as pd
from keras.layers import Dense, LSTM
import matplotlib.pyplot as plt
import tensorflow as tf

# Membaca dataset awal
df= pd.read_csv('/content/LSTM-Multivariate_pollution.csv')
df

# Menghapus kolom yang tidak diperlukan
df = df.drop(columns = ['dew', 'temp', 'pollution', 'wnd_dir', 'wnd_spd', 'snow', 'rain'])

# Mengecek data
df.info()

"""## Pemrosesan Lanjutan Data untuk Machine Learning
Selanjutnya dilakukan proses perubahan nilai-nilai dari dataframe ke dalam tipe data numpy array menggunakan atribut values menghasilkan dates dan press. Data array dates dan press dapat ditampilkan dalam sebuah grafik.  Untuk memudahkan model machine learning mencapai convergen, dilakukan normalisasi dengan fungsi MinMaxScaler dari library SKLearn. Agar dapat digunakan press harus diubah ukurannya dengan atribut reshape(-1,1). Nilai press hasil normalisasi disimpan pada press_scaled. Ukuran press_scaled berbeda dengan dates sehingga press_scaled diubah menjadi sebuah dataframe kembali lalu kolom press diekstrak menjadi sebuah array press_final. Sedangkan dates_final nilainya sama dengan dates. Selanjutnya dilakukan pembagian data training dan data validation dengan menggunakan train_test_split pada library SKLearn menjadi 80% data training dan 20% data validation. Langkah terakhir membuat fungsi  windowed_dataset yang menerima sebuah series/atribut kita yang telah di konversi menjadi tipe numpy, lalu mengembalikan label dan atribut dari dataset dalam bentuk batch.Fungsi ini diterapkan pada press_train dan press_val membentuk train_set dan val_set.
"""

# Mengubah nilai-nilai dari dataframe ke dalam tipe data numpy array menggunakan atribut values.
dates = df['date'].values
press = df['press'].values

# Grafik Tekanan udara berdasarkan array dates, press
plt.figure(figsize=(15,5))
plt.plot(dates, press)
plt.title('Tekanan Udara',
          fontsize=20);

# Normalisasi dengan MinMaxScaler pada data press
from sklearn.preprocessing import MinMaxScaler
min_max_scaler = MinMaxScaler()
press_scaled = min_max_scaler.fit_transform(press.reshape(-1,1))
press_scaled

# Membuat press_final yang ukurannya sama dengan dates_final
dates_final = dates
press_final = pd.DataFrame(press_scaled, columns =['press'])
press_final = press_final['press'].values

#Membagi data menjadi data train dan data validation
from sklearn.model_selection import train_test_split
date_train, date_val, press_train, press_val = train_test_split(dates_final, press_final, test_size=0.2, shuffle=False)
print(date_train.shape, date_val.shape, press_train.shape, press_val.shape)

# Membuat fungsi windowed_dataset
def windowed_dataset(series, window_size, batch_size, shuffle_buffer):
    series = tf.expand_dims(series, axis=-1)
    ds = tf.data.Dataset.from_tensor_slices(series)
    ds = ds.window(window_size + 1, shift=1, drop_remainder=True)
    ds = ds.flat_map(lambda w: w.batch(window_size + 1))
    ds = ds.shuffle(shuffle_buffer)
    ds = ds.map(lambda w: (w[:-1], w[-1:]))
    return ds.batch(batch_size).prefetch(1)

# Menerapkan fungsi windowed_dataset ke press_train dan press_val
train_set = windowed_dataset(press_train, window_size=60, batch_size=100, shuffle_buffer=1000)
val_set = windowed_dataset(press_val, window_size=60, batch_size=100, shuffle_buffer=1000)

"""## Membuat dan Menguji Model Machine Learning dengan Data Time Series
Proses yang pertama kali dilakukan adalah membuat model sekuensial. Model sekuensial yang digunakan pada proyek ini terdiri dari layer LSTM, Dropout, Dense dengan activation relu dan softmax. Layer LSTM berguna untuk mengurutkan data dengan layer pertama LSTM memiliki parameter return_sequences yang bernilai True. Layer Dropout digunakan untuk menghapus beberapa nodes secara acak sehingga dapat mencegah terjadinya overfitting. Beberapa Layer Dense relu merupakan hidden layer yang berisi algoritma dari ML. Sedangkan layer Dense softmax merupakan layer untuk output.


Setelah model ML berhasil dibuat, model kemudian dicompile dengan menggunakan fungsi loss Huber. Optimizer yang digunakan adalah SGD yang menghitung penurunan gradien (dengan momentum dan parameter learning rate). Model compile ini akan mengitung parameter MAE (Mean Absolute error).

Terdapat satu kriteria yang mengharuskan model memiliki nilai MAE < 10% skala data. Sehingga dilakukan perhitungan nilai ambang batas (threshold_mae) yang tepat untuk data press_final. Diperoleh nilai threshold_mae 0,1.

Selanjutnya model ML dilatih dengan menggunakan fungsi fit sebanyak 100 kali epoch. Fungsi fit ini akan berhenti karena adanya Callback sudah mencapai MAE training dan MAE validation kurang dari threshold_mae(diatur 0,05).
Model fit ini disimpan dalam variable history sehingga dapat ditampilkan dalam sebuah grafik MAE dan Loss dari data training dan data validation menggunakan library matplotlib.
"""

# Model sequential dengan 2 buah LSTM,
model = tf.keras.models.Sequential([
  tf.keras.layers.LSTM(128,return_sequences=True),
  tf.keras.layers.LSTM(128),
  tf.keras.layers.Dropout(0.4),
  tf.keras.layers.Dense(64, activation="relu"),
  tf.keras.layers.Dense(32, activation="relu"),
  tf.keras.layers.Dense(1),
])

# Model compile
optimizer = tf.keras.optimizers.SGD(learning_rate=1.0000e-04, momentum=0.9)
model.compile(loss=tf.keras.losses.Huber(),
              optimizer=optimizer,
              metrics=["mae"])

threshold_mae = (press_final.max() - press_final.min()) * 10/100
threshold_mae

# Membuat class callback
class myCallback(tf.keras.callbacks.Callback):
  def on_epoch_end(self, epoch, logs={}):
    if(logs.get('mae')<0.05 and logs.get('val_mae')<0.05):
      print("MAE dari model < 10% skala data")
      self.model.stop_training = True

callbacks = myCallback()

# Model Fit
history = model.fit(train_set, epochs=100,
                    validation_data= val_set, verbose=2, callbacks=[callbacks])

# Grafik
import matplotlib.pyplot as plt
# Grafik MAE
plt.plot(history.history['mae'])
plt.plot(history.history['val_mae'])
plt.title('MAE Model Machine Learning dengan Data Time Series Tekanan Udara')
plt.ylabel('mae')
plt.xlabel('epoch')
plt.ylim(ymin=0)
plt.legend(['train', 'validation'], loc='upper left')
plt.show()

# Grafik Loss
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Loss Model Machine Learning dengan Data Time Series Tekanan Udara')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'validation'], loc='upper left')
plt.show()

